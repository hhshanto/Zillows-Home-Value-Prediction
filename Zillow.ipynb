{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4847965",
   "metadata": {},
   "source": [
    "# Zillow’s Home Value Prediction (Zestimate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7aa1037",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3df66a53",
   "metadata": {},
   "source": [
    "Zillow’s Zestimate home valuation has shaken up the U.S. real estate industry since first released 11 years ago.\n",
    "\n",
    "A home is often the largest and most expensive purchase a person makes in his or her lifetime. Ensuring homeowners have a trusted way to monitor this asset is incredibly important. The Zestimate was created to give consumers as much information as possible about homes and the housing market, marking the first time consumers had access to this type of home value information at no cost.\n",
    "\n",
    "“Zestimates” are estimated home values based on 7.5 million statistical and machine learning models that analyze hundreds of data points on each property. And, by continually improving the median margin of error (from 14% at the onset to 5% today), Zillow has since become established as one of the largest, most trusted marketplaces for real estate information in the U.S. and a leading example of impactful machine learning.\n",
    "\n",
    "Zillow Prize, a competition with a one million dollar grand prize, is challenging the data science community to help push the accuracy of the Zestimate even further. Winning algorithms stand to impact the home values of 110M homes across the U.S.\n",
    "\n",
    "In this million-dollar competition, participants will develop an algorithm that makes predictions about the future sale prices of homes. The contest is structured into two rounds, the qualifying round which opens May 24, 2017 and the private round for the 100 top qualifying teams that opens on Feb 1st, 2018. In the qualifying round, you’ll be building a model to improve the Zestimate residual error. In the final round, you’ll build a home valuation algorithm from the ground up, using external data sources to help engineer new features that give your model an edge over the competition.\n",
    "\n",
    "Because real estate transaction data is public information, there will be a three-month sales tracking period after each competition round closes where your predictions will be evaluated against the actual sale prices of the homes. The final leaderboard won’t be revealed until the close of the sales tracking period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e2ab60d",
   "metadata": {},
   "source": [
    "`Link`: https://www.kaggle.com/competitions/zillow-prize-1/overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72c473cb",
   "metadata": {},
   "source": [
    "## Evaluation: \n",
    "Submissions are evaluated on Mean Absolute Error between the predicted log error and the actual log error.\n",
    "The log error is defined as   \n",
    "**logerror = log(Zestimate) - log(SalePrice)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd70acd",
   "metadata": {},
   "source": [
    "## Dataset Description:\n",
    "`Train/Test split`\n",
    "\n",
    "There are full list of real estate properties in three counties (Los Angeles, Orange and Ventura, California) data in 2016.\n",
    "\n",
    "The train data has all the transactions before October 15, 2016, plus some of the transactions after October 15, 2016.\n",
    "\n",
    "The test data in the public leaderboard has the rest of the transactions between October 15 and December 31, 2016.\n",
    "\n",
    "The rest of the test data, which is used for calculating the private leaderboard, is all the properties in October 15, 2017, to December 15, 2017. This period is called the \"sales tracking period\", during which we will not be taking any submissions.\n",
    "\n",
    "We have to predict 6 time points for all properties: October 2016 (201610), November 2016 (201611), December 2016 (201612), October 2017 (201710), November 2017 (201711), and December 2017 (201712).\n",
    "\n",
    "Not all the properties are sold in each time period. If a property was not sold in a certain time period, that particular row will be ignored when calculating score.\n",
    "\n",
    "If a property is sold multiple times within 31 days, we take the first reasonable value as the ground truth. By \"reasonable\", we mean if the data seems wrong, we will take the transaction that has a value that makes more sense."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f58412",
   "metadata": {},
   "source": [
    "## File description:\n",
    "**properties_2016.csv** - all the properties with their home features for 2016. Note: Some 2017 new properties don't have any data yet except for their parcelid's. Those data points should be populated when properties_2017.csv is available.  \n",
    "**properties_2017.csv** - all the properties with their home features for 2017 (released on 10/2/2017)  \n",
    "**train_2016.csv** - the training set with transactions from 1/1/2016 to 12/31/2016  \n",
    "**train_2017.csv** - the training set with transactions from 1/1/2017 to 9/15/2017 (released on 10/2/2017)  \n",
    "**sample_submission.csv** - a sample submission file in the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0770d1bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../Zillows-Home-Value-Prediction/Data\\properties_2016.csv\n",
      "../Zillows-Home-Value-Prediction/Data\\properties_2017.csv\n",
      "../Zillows-Home-Value-Prediction/Data\\sample_submission.csv\n",
      "../Zillows-Home-Value-Prediction/Data\\train_2016_v2.csv\n",
      "../Zillows-Home-Value-Prediction/Data\\train_2017.csv\n",
      "../Zillows-Home-Value-Prediction/Data\\zillow-prize-1.zip\n",
      "../Zillows-Home-Value-Prediction/Data\\zillow_data_dictionary.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Import necessary librairies\n",
    "import gc \n",
    "import numpy as np # linear algebra\n",
    "from numpy import hstack\n",
    "from numpy import array\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt  # Matlab-style plotting\n",
    "import seaborn as sns\n",
    "color = sns.color_palette()\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "# Datetime operations\n",
    "import time\n",
    "\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew #for some statistics\n",
    "\n",
    "# Definitions\n",
    "pd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n",
    "pd.set_option('display.max_columns', 100)    #Display upto 100 columns \n",
    "pd.set_option('display.max_rows', 100) \n",
    "%matplotlib inline\n",
    "\n",
    "# Check the files available in the directory\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('../Zillows-Home-Value-Prediction/Data'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# Suppress warnings \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98258310",
   "metadata": {},
   "source": [
    "## Importing Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8287b429",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "HOUSING_PATH = os.path.join(\"..\", \"Zillows-Home-Value-Prediction\", \"Data\")\n",
    "PROPERTIES_2016 = 'properties_2016.csv'\n",
    "PROPERTIES_2017 = 'properties_2017.csv'\n",
    "TRAIN_2016 = 'train_2016_v2.csv'\n",
    "TRAIN_2017 = 'train_2017.csv'\n",
    "\n",
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "    properties_2016 = pd.read_csv(os.path.join(housing_path, PROPERTIES_2016))\n",
    "    properties_2017 = pd.read_csv(os.path.join(housing_path, PROPERTIES_2017))\n",
    "    train_2016 = pd.read_csv(os.path.join(housing_path, TRAIN_2016))\n",
    "    train_2017 = pd.read_csv(os.path.join(housing_path, TRAIN_2017))\n",
    "\n",
    "    # Left join will ignore all properties that do not have a logerror (target variable) associated with them\n",
    "    train_2016 = pd.merge(train_2016, properties_2016, how = 'left', on = 'parcelid')\n",
    "    train_2017 = pd.merge(train_2017, properties_2017, how = 'left', on = 'parcelid')\n",
    "    \n",
    "    # Union data for 2016 and 2017 into one dataframe\n",
    "    all_properties = pd.concat([properties_2016, properties_2017], ignore_index=True)\n",
    "    all_training = pd.concat([train_2016, train_2017], ignore_index=True)\n",
    "    return all_properties, all_training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31bcac49",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_properties, housing = load_housing_data()\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f946d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checking the time to read data\n",
    "start = time.time()\n",
    "tempDf = pd.read_csv('../Zillows-Home-Value-Prediction/Data/properties_2016.csv')\n",
    "train = pd.read_csv(\"../Zillows-Home-Value-Prediction/Data/train_2016_v2.csv\", parse_dates=[\"transactiondate\"])\n",
    "dfTrain= train.merge(tempDf, how=\"left\", on = \"parcelid\")\n",
    "end= time.time()\n",
    "\n",
    "print(\"Time taken: {}\".format(end-start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b537db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Checking memory usage\n",
    "train.info(verbose=False), tempDf.info(verbose=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f611d3",
   "metadata": {},
   "source": [
    "Looks good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e479fa4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The shape {}\".format(all_properties.shape))\n",
    "\n",
    "all_properties.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a49f439",
   "metadata": {},
   "source": [
    "**Understanding:**  \n",
    "There are total 5970434 or 5.9 Million properties available in the file.  \n",
    "**Variable types distribution:** 53 Float64, 6 Object (Categorical) and 1 Integer (ParcelID)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6def14",
   "metadata": {},
   "source": [
    "Check the NaN value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da898f25",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#all_properties.isna().sum()\n",
    "# Checking what percentage of every column is missing\n",
    "percent_missing = all_properties.isnull().sum() * 100 / len(all_properties)\n",
    "\n",
    "percent_missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c1b752",
   "metadata": {},
   "outputs": [],
   "source": [
    "moreThan90Per = percent_missing>90\n",
    "moreThan90Per.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8199d6",
   "metadata": {},
   "source": [
    "20 columns have more than 90% data missing  \n",
    "24 columns have more than 80% data missing  \n",
    "29 columns have more than 50% data missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de4e7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "09671084",
   "metadata": {},
   "source": [
    "### Duplicate Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04489707",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613ae2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = housing.logerror\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811bb1bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.hist(bins=100, figsize=(8,5))\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
